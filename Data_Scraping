import requests
import random
import pandas as pd
import urllib
import bs4 as bs



headers_list = [
    # Firefox 77 Mac
    {
    "User-Agent": "Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:77.0) Gecko/20100101 Firefox/77.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Referer": "https://www.google.com/",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
    },
    # Chrome 92.0 Win10
    {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/92.0.4515.107 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8",
    "Accept-Language": "en-US,en;q=0.5",
    "Accept-Encoding": "gzip, deflate, br",
    "Referer": "https://www.google.com/",
    "DNT": "1",
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1"
    },
    # Chrome 91.0 Win10
    {
    "Connection": "keep-alive",
    "DNT": "1",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
    "Sec-Fetch-Site": "none",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-Dest": "document",
    "Referer": "https://www.google.com/",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "en-GB,en-US;q=0.9,en;q=0.8"
    },
    # Firefox 90.0 Win10
    {
    "Connection": "keep-alive",
    "Upgrade-Insecure-Requests": "1",
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:90.0) Gecko/20100101 Firefox/90.0",
    "Accept": "text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,image/apng,*/*;q=0.8,application/signed-exchange;v=b3;q=0.9",
    "Sec-Fetch-Site": "same-origin",
    "Sec-Fetch-Mode": "navigate",
    "Sec-Fetch-User": "?1",
    "Sec-Fetch-Dest": "document",
    "Referer": "https://www.google.com/",
    "Accept-Encoding": "gzip, deflate, br",
    "Accept-Language": "en-US,en;q=0.9"
    }
]






cities=['seattle','chicago','newyork','newjersey','boston','austin']


joblists=[]
#urls=['https://tippecanoe.craigslist.org/d/jobs/search/jjj','https://tippecanoe.craigslist.org/d/jobs/search/jjj?s=120','https://tippecanoe.craigslist.org/d/jobs/search/jjj?s=240','https://tippecanoe.craigslist.org/d/jobs/search/jjj?s=360']
for city in cities:
    for n in range(25):
        url='https://'+city+'.craigslist.org/d/jobs/search/jjj?s='+str(n*120)
        headers = random.choice(headers_list)
        r = requests.Session()
        r.headers = headers
        #time.sleep(np.random.uniform(5,15))
        html = r.get(url).text
        soup=bs.BeautifulSoup(html, 'html.parser')
        tablef=soup.find_all('h3',class_="result-heading")
        for i in range(len(tablef)):
            joblists.append(tablef[i].a['href'])
            
            
  
job_postings=[]
for listing in joblists:
    headers = random.choice(headers_list)
    r = requests.Session()
    r.headers = headers
    html = r.get(listing).text
    soup=bs.BeautifulSoup(html, 'html.parser')
    text1=soup.get_text()
    start=text1.find('print')
    end=text1.find('posted:')
    job_postings.append(text1[start+5:end+20])
    
    
    
    
    
    
cities=['seattle','chicago']
resumelists=[]
#urls=['https://tippecanoe.craigslist.org/d/resumes/search/rrr']
for city in cities:
    for n in range(10):
        url='https://'+city+'.craigslist.org/d/resumes/search/rrr?s='+str(n*120)
        headers = random.choice(headers_list)
        r = requests.Session()
        r.headers = headers
        #time.sleep(np.random.uniform(5,15))
        html = r.get(url).text
        soup=bs.BeautifulSoup(html, 'html.parser')
        page=soup.find('span',class_="button pagenum")
        if page == None : 
            pass
        else :
            pnum=page.get_text()
            if pnum!="no results":
                tablef=soup.find_all('h3',class_="result-heading")
                for i in range(len(tablef)):
                    resumelists.append(tablef[i].a['href'])
                    

resume_postings=[]
for listing in resumelists:
    headers = random.choice(headers_list)
    r = requests.Session()
    r.headers = headers
    html = r.get(listing).text
    soup=bs.BeautifulSoup(html, 'html.parser')
    text1=soup.find('section',id="postingbody")    
    resume_postings.append(text1.get_text())























